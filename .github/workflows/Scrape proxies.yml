# This workflow will install Python dependencies, run tests and lint with a single version of Python
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python

name: Scrape and check proxies

on:
  schedule:
    - cron: '*/30 * * * *' # run every 30 minutes
#   - cron: '0 * * * *' # run every 60 minutes
  workflow_dispatch: # be able to optionally run it manually

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.12.9
      uses: actions/setup-python@v3
      with:
        python-version: "3.12.9"
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        # the directory that should be cached
        path: ~/.cache/pip
        # A key to identify the cache. It's created from the runner's OS and a hash of requirements.txt.
        # If requirements.txt changes, a new cache will be created.
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        # A fallback key to use if the primary key doesn't have a cache hit.
        restore-keys: |
          ${{ runner.os }}-pip-
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Run the proxy scraper
      run: |
        python ScrapeAllProxies.py --output proxies/scraped-proxies.txt --exclude Webshare --verbose
    - name: Run the proxy checker
      run: |
        python CheckProxies.py --input proxies/scraped-proxies.txt proxies/working-proxies-all.txt --output proxies/working-proxies.txt --threads 3000 --timeout 3s
    # commit the changes
    - name: Commit and push
      uses: stefanzweifel/git-auto-commit-action@v5
      with:
        commit_message: "Re-scrape proxies"
        file_pattern: "proxies/scraped-proxies.txt proxies/working-proxies-*.txt" # only commit the matching files
